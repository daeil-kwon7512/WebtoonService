import requests
import pandas as pd
from tqdm import tqdm
import time
import os
from datetime import datetime

# í—¤ë” ì„¤ì •
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
}

# íŒŒì¼ ê²½ë¡œ ì •ì˜
FILES = {
    'webtoon': "data_webtoon.csv",
    'author': "data_author.csv",
    'genre': "data_genre.csv",
    'tag': "data_tag.csv"
}

def get_today_naver_code():
    """ì˜¤ëŠ˜ ìš”ì¼ì„ ë„¤ì´ë²„ API íŒŒë¼ë¯¸í„°(mon, tue...)ë¡œ ë³€í™˜"""
    # ì›”=0, í™”=1, ..., ì¼=6
    weekday_idx = datetime.now().weekday()
    codes = ['mon', 'tue', 'wed', 'thu', 'fri', 'sat', 'sun']
    return codes[weekday_idx]

def load_existing_data():
    """ê¸°ì¡´ CSV íŒŒì¼ì´ ìˆìœ¼ë©´ ë¶ˆëŸ¬ì˜¤ê³ , ì—†ìœ¼ë©´ ë¹ˆ DataFrame ë°˜í™˜"""
    dfs = {}
    for key, path in FILES.items():
        if os.path.exists(path):
            dfs[key] = pd.read_csv(path)
        else:
            # íŒŒì¼ì´ ì—†ìœ¼ë©´ ë¹ˆ í”„ë ˆì„ ìƒì„± (ì»¬ëŸ¼ êµ¬ì¡°ëŠ” ë‚˜ì¤‘ì— concatí•  ë•Œ ë§ì¶°ì§)
            dfs[key] = pd.DataFrame()
    return dfs

def crawl_naver_webtoon_incremental():
    # 1. ê¸°ì¡´ ë°ì´í„° ë¡œë“œ
    existing_dfs = load_existing_data()
    
    # 2. ìˆ˜ì§‘í•  ìš”ì¼ ê²°ì •
    if existing_dfs['webtoon'].empty:
        print("ğŸš€ [ì´ˆê¸° ì‹¤í–‰] ë°ì´í„° íŒŒì¼ì´ ì—†ì–´ 'ëª¨ë“  ìš”ì¼'ì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤.")
        target_days = ['mon', 'tue', 'wed', 'thu', 'fri', 'sat', 'sun', 'dailyPlus']
        is_initial_run = True
        existing_ids = set()
    else:
        today = get_today_naver_code()
        print(f"ğŸ”„ [ì¦ë¶„ ì‹¤í–‰] ê¸°ì¡´ ë°ì´í„° ë°œê²¬! 'ì˜¤ëŠ˜({today})' ì—°ì¬ì‘ë§Œ í™•ì¸í•©ë‹ˆë‹¤.")
        target_days = [today, 'dailyPlus'] # ì˜¤ëŠ˜ì€ í•„ìˆ˜, dailyPlus(ë§¤ì¼+)ë„ í™•ì¸ ì¶”ì²œ
        is_initial_run = False
        existing_ids = set(existing_dfs['webtoon']['titleId'].unique())

    # ---------------------------------------------------------
    # 3. ê¸°ë³¸ ë¦¬ìŠ¤íŠ¸ ìˆ˜ì§‘ (íƒ€ê²Ÿ ìš”ì¼ë§Œ)
    # ---------------------------------------------------------
    print("=== 1. ê¸°ë³¸ ì›¹íˆ° ëª©ë¡ ìˆ˜ì§‘ ===")
    
    new_webtoon_list = []
    new_author_list = []
    
    collected_ids = set() # ì´ë²ˆ ì‹¤í–‰ì—ì„œ ë°œê²¬í•œ IDë“¤

    for day_code in tqdm(target_days):
        url = f'https://comic.naver.com/api/webtoon/titlelist/weekday?week={day_code}&order=user'
        try:
            response = requests.get(url, headers=headers)
            if response.status_code == 200:
                data = response.json()
                
                for webtoon in data["titleList"]:
                    t_id = int(webtoon["titleId"])
                    collected_ids.add(t_id)

                    # ì´ë¯¸ ìˆëŠ” ì›¹íˆ°ì´ë©´ ê¸°ë³¸ ì •ë³´ë§Œ ì—…ë°ì´íŠ¸í•˜ê³  ìŠ¤í‚µí•  ìˆ˜ë„ ìˆìŒ
                    # ì—¬ê¸°ì„œëŠ” 'ìƒˆë¡œìš´ ì›¹íˆ°'ë§Œ ë¦¬ìŠ¤íŠ¸ì— ë‹´ì•„ì„œ ì²˜ë¦¬
                    if t_id in existing_ids:
                        continue 

                    # === ì‹ ê·œ ì›¹íˆ° ë°œê²¬! ===
                    # 1-1. ë©”ì¸ ì •ë³´
                    new_webtoon_list.append({
                        "titleId": t_id,
                        "titleName": webtoon["titleName"],
                        "thumbnailUrl": webtoon["thumbnailUrl"],
                        "starScore": float(webtoon["starScore"]),
                        "viewCount": int(webtoon["viewCount"]),
                        "adult": webtoon["adult"],
                        "finish": webtoon["finish"]
                    })
                    
                    # 1-2. ì‘ê°€ ì •ë³´
                    writers = [w['name'] for w in webtoon.get("writers", [])]
                    painters = [p['name'] for p in webtoon.get("painters", [])]
                    origins = [o['name'] for o in webtoon.get("novelOriginAuthors", [])]
                    
                    all_authors = list(set(writers + painters + origins))
                    for name in all_authors:
                        new_author_list.append({"titleId": t_id, "authorName": name})
                        
            time.sleep(0.1)
        except Exception as e:
            print(f"Error crawling {day_code}: {e}")

    print(f"ğŸ‘‰ ê¸ˆì¼ í™•ì¸ëœ ì „ì²´ ID ìˆ˜: {len(collected_ids)}")
    print(f"ğŸ‘‰ ìƒˆë¡œ ì¶”ê°€ë  ì‹ ê·œ ì›¹íˆ°: {len(new_webtoon_list)}ê°œ")

    if not new_webtoon_list:
        print("âœ… ì‹ ê·œ ì›¹íˆ°ì´ ì—†ìŠµë‹ˆë‹¤. ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return

    # ì‹ ê·œ ë°ì´í„°í”„ë ˆì„ ìƒì„±
    df_new_webtoon = pd.DataFrame(new_webtoon_list)
    df_new_author = pd.DataFrame(new_author_list)

    # ---------------------------------------------------------
    # 4. ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ (ì‹ ê·œ ì›¹íˆ°ë§Œ!)
    # ---------------------------------------------------------
    print("\n=== 2. ì‹ ê·œ ì›¹íˆ° ìƒì„¸ íƒœê·¸/ì¥ë¥´ ìˆ˜ì§‘ ===")
    
    genre_list = []
    tag_list = []
    
    session = requests.Session()
    session.headers.update(headers)
    
    # ìƒˆë¡œ ì°¾ì€ ì›¹íˆ°ì˜ ID ëª©ë¡ë§Œ ìˆœíšŒ
    new_ids = df_new_webtoon['titleId'].unique()
    
    for t_id in tqdm(new_ids):
        url = f'https://comic.naver.com/api/article/list/info?titleId={t_id}'
        try:
            response = session.get(url)
            if response.status_code == 200:
                data = response.json()
                gfp_data = data.get("gfpAdCustomParam", {})
                
                # ì¥ë¥´
                if gfp_data.get("genreTypes"):
                    for g in gfp_data["genreTypes"]:
                        genre_list.append({"titleId": t_id, "genre": g})
                
                # íƒœê·¸
                if gfp_data.get("tags"):
                    for t in gfp_data["tags"]:
                        tag_list.append({"titleId": t_id, "tag": t})
            
            # time.sleep(0.05)
            
        except Exception as e:
            print(f"Error detail {t_id}: {e}")

    df_new_genre = pd.DataFrame(genre_list)
    df_new_tag = pd.DataFrame(tag_list)
    
    # ---------------------------------------------------------
    # 5. ë°ì´í„° ë³‘í•© ë° ì €ì¥
    # ---------------------------------------------------------
    print("\n=== 3. ë°ì´í„° ë³‘í•© ë° ì €ì¥ ===")

    # ê¸°ì¡´ ë°ì´í„°ì™€ í•©ì¹˜ê¸° (concat)
    final_dfs = {}
    
    # (1) Webtoon
    if not existing_dfs['webtoon'].empty:
        # í˜¹ì‹œ ëª¨ë¥¼ ì¤‘ë³µ ì œê±° (ê¸°ì¡´ ê²ƒ ìœ ì§€, ìƒˆê²ƒ ì¶”ê°€)
        final_dfs['webtoon'] = pd.concat([existing_dfs['webtoon'], df_new_webtoon]).drop_duplicates(subset=['titleId'], keep='last')
    else:
        final_dfs['webtoon'] = df_new_webtoon

    # (2) Author
    if not existing_dfs['author'].empty:
        final_dfs['author'] = pd.concat([existing_dfs['author'], df_new_author]).drop_duplicates()
    else:
        final_dfs['author'] = df_new_author

    # (3) Genre
    if not existing_dfs['genre'].empty:
        final_dfs['genre'] = pd.concat([existing_dfs['genre'], df_new_genre]).drop_duplicates()
    else:
        final_dfs['genre'] = df_new_genre
        
    # (4) Tag
    if not existing_dfs['tag'].empty:
        final_dfs['tag'] = pd.concat([existing_dfs['tag'], df_new_tag]).drop_duplicates()
    else:
        final_dfs['tag'] = df_new_tag

    # íŒŒì¼ë¡œ ì €ì¥
    for key, path in FILES.items():
        if not final_dfs[key].empty:
            final_dfs[key].to_csv(path, index=False)
            print(f"- {path} ì €ì¥ ì™„ë£Œ ({len(final_dfs[key])}í–‰)")
        else:
            print(f"- {key} ë°ì´í„°ê°€ ì—†ì–´ ì €ì¥í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

    print("\nğŸ‰ í¬ë¡¤ë§ ë° ë°ì´í„° ì—…ë°ì´íŠ¸ ì™„ë£Œ!")

if __name__ == "__main__":
    crawl_naver_webtoon_incremental()